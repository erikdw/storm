 feature | Storm-0.11.0-SNAPSHOT (and expected pull requests) | JStorm-2.1.0 | Notes | JIRA |
----------|-----------------------|--------------|-------|----|
 Security | Basic Authentication/Authorization | N/A | The migration is not high risk. But JStorm daemons/connections need to be evaluated
Scheduler | Resource aware scheduling	(RAS) (Work In Progress) | <ol><li>Evenly distribute a component's tasks across the nodes in the cluster.<li>Balanace the number of tasks in a worker.<li>Try to assign two tasks which are transferring messages directly into the same worker to reduce network cost.<li>Support user-defined assignment and using the result of the last assignment.	Different solution between Storm and JStorm.</ol> Actually, JStorm has always used cpu/mem/disk/network demand for scheduling. But we faced the problem of low utilization of cluster. For example, because the configuration of topologies and hardware resources in a cluster are different. After running a period, it is possible that a node will satisfy almost all of the resource demands, but lack 1 resource (e.g., having 4 CPUs but the topology needing 5 CPUs). | Scheduler interface is pluggable so we should be able to support boht schedulers for the time being |
Nimbus HA	| Support for a pool of nimbus servers.  Once Blobstore is merged in, "leader election" and "state storage" will be separate. | Support to configure more than one backup nimbus. When the master nimbus is down, the most appropriate spare nimbus (topologies on disk most closely match the records in ZooKeeper) will be chosen to be promoted. | Different solution.| |
Topology structure | worker/executor/task | worker/task | This might be the biggest difference between Storm and JStorm. Not sure if there are some features related to "executor" that cannot be merged. (Rebalancing allows JStorm to combine multiple tasks on a single executor)  This may go away in the future once RAS is stable. |
Topology Master | Heartbeat server is similar for scaling.  Change is under review. | New system bolt "topology master" was added, which is responsible for collecting task heartbeat info of all tasks and reporting the info to nimbus. Besides task heartbeat info, it also can be used to dispatch control messages within the topology. Topology master significantly reduces the amout of read/write to ZooKeeper.	Before this change, ZooKeeper was the bottleneck for deploying big clusters and topologies.
Backpressure | Store backpressure status in ZooKeeper, which can trigger the source spout to start "flow control", meaning it will stop sending tuples. | <ol><li>Implement backpressure using "topology master" (TM). TM is responsible for processing the trigger message and sending the flow control request to relevant spouts.  "flow control" in JStorm doesn't complete stop the spout from emitting tuples, but instead just slows down the tuple sending.<li>User can update the configuration of backpressure dynamically without restarting topology, e.g. enable/disable backpressue, high/low watermark, etc. | These two solutions are totally different, we need to evaluate which one is better. |
Monitoring of task execute thread | N/A | Monitors the status of the execute thread of tasks. It is effective to find the slow bolt in a topology, and potentially uncovers deadlock as well. | 	
Message processing | Configurable multi-receiving threads of worker (but likely to change to have deserialization happen in Netty) | <ol><li>Add receiving and transferring queue/thread for each task to make deserialization and serialization asynchronously<li>Remove receiving and transferring thread on worker level to avoid unnecessary locks and to shorten the message processing phase | Low risk for merging to JStorm (Both implementations are similar) |
Batch tuples | Batching in DisruptorQueue | Do batch before sending tuple to transfer queue and support for adjusting the batch size dynamically according to samples of actual batch size sent out for past intervals. | Should evaluate both implementations, and see which is better, or whether we can move some of the dynamic batching logic into the disruptor level |	
Grouping | Load aware balancing shuffle grouping | <ol><li>Has a "localfirst" grouping that causes tuples to be sent to the tasks in the same worker by default. But if the load of all local tasks is high, the tuples will be sent out to remote tasks.<li>Improve localOrShuffle grouping that tuples are only sent to the tasks in the same worker or in same node.	The shuffle of JStorm checks the load of network connection of local worker. (QUESTION: this item isn't clear.)| The risk for merging of checking remote load which was implemented in "Load aware balancing shuffle grouping" is low.
REST API | Yes | NA |The porting of REST API to JStorm is in progress
web-UI | Yes | <ol><li>Has a redesigned Web-UI with clear and clean code.<li>Data display is much more beautiful and provides a better user experience, such as the topology graph and 30 minute metric trend. Example URL: http://storm.taobao.org/	
metric system | | <ol><li>All levels of metrics, including stream metrics, task metrics, component metrics, topology metrics, even cluster metrics, are sampled & calculated. Some metrics, e.g. ""tuple life cycle"", are very useful for debugging and finding the hotspots of a topology.<li>Support full metrics data. Previous metric system can only display mean value of meters/histograms, the new metric system can display m1, m5, m15 of meters, and common percentiles of histograms.<li>Use new metrics windows, the mininum metric window is 1 minute, thus we can see the metrics data every single minute.<li>Supplies a metric uploader interface, third-party companies can easily build their own metric systems based on the historic metric data. | JStorm metric system is completely redesigned.
jar command | Support progress bar when submitting topology | Does not support progress bar | Low risk for merging to JStorm
rebalance	command | Basic functionality of rebalance | Besides rebalance, scale-out/in by updating the number of workers, ackers, spouts & bolts dynamically without stopping topology.  Routing is updated dynamically within upstream components. | dynamic routing with some groupings is difficult to get right with state, we need to be sure this is well documented, and might want to disallow it for some groupings without a force flag. | 
list command | List information of topologies| List information of all topologies, all supervisors, and JStorm version |
zktool command | N/A | Supports some ZooKeeper operations, e.g. "list", "read"â€¦	| will need to be evaluated for security |
metricsMonitor command | monitor is similar, but read-only | Allows toggling on/off some metrics which may impact topology performance |
restart command | N/A | Restart a topology. Besides restart, this commond can also update the topology configuration. |
update_topology command | N/A | Update jars and configuration dynamically for a topology, without stopping the topology. |
shell	command | | N/A | Low risk for merging to JStorm
ui command | | N/A | Low risk for merging to JStorm
logviewer command	| | N/A | Low risk for merging to JStorm
repl command | | N/A | No need to merge this for java core
cgroup | with storm-on-mesos framework, Storm can restrict the CPU and Memory available to a worker process | Support the upper limit control of cpu core usage for a worker by cgroup |	
Log	| <ol><li>Support user-defined log configuration via logback (0.9.x) and Log4j 2 in storm 0.10.x+ | <li>support dynamic changes to logging of a running topology</ol>|<ol><li>Support user-defined configuration of log<li>Support both logback and log4j</ol>|
Worker classloader | N/A uses shading for most dependencies | Avoid problem of re-loading classes |
blobstore (coming soon) | Distributed cache-like interface with dynamic updating without bringing down a topology | N/A |
